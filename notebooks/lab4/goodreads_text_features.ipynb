{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c702c2-fdc5-48bb-aa51-25fcbc42b15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "storage_acct = \"goodreadsreviews60107070\"\n",
    "\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.goodreadsreviews60107070.dfs.core.windows.net\",\n",
    "    \"W68YOwummMkTfxvE8uSyeHwSn2ISU3fxF43SpgTIU/zdUDmwquZ95QpaxDJnze6PRovNww3bWamU+AStmbtZLg==\"\n",
    ")\n",
    "\n",
    "container   = \"lakehouse\"\n",
    "silver_path = f\"abfss://{container}@{storage_acct}.dfs.core.windows.net/processed\"\n",
    "gold_path   = f\"abfss://{container}@{storage_acct}.dfs.core.windows.net/gold\"\n",
    "\n",
    "# Output locations for splits\n",
    "features_v2_base = f\"{gold_path}/features_v2\"\n",
    "train_out = f\"{features_v2_base}/train\"\n",
    "val_out   = f\"{features_v2_base}/val\"\n",
    "test_out  = f\"{features_v2_base}/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b3fcb3-3f77-405e-a349-fd6011e53af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 1) LOAD CLEAN SOURCE ===\n",
    "# Use features_v1 as the leakage-safe source for downstream featurization\n",
    "df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(f\"{gold_path}/features_v1\")  # load by path instead of table()\n",
    ")\n",
    "\n",
    "# --- Optional: Basic hygiene filters ---\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = (\n",
    "    df.dropDuplicates([\"review_id\"])\n",
    "      .filter(F.col(\"review_text\").isNotNull() & (F.length(F.col(\"review_text\")) >= 10))\n",
    "      .filter(F.col(\"rating\").isNotNull())\n",
    ")\n",
    "\n",
    "# Verify schema and sample\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542fb74f-2fe6-4eda-8c68-d05b8b29dce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 2) MAKE REPRODUCIBLE SPLITS (70/15/15) ===\n",
    "# Note: split BEFORE TF-IDF or encoders to avoid data leakage\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "seed = 67\n",
    "\n",
    "splits = (\n",
    "    df.withColumn(\"_rand\", F.rand(seed))\n",
    "      .withColumn(\n",
    "          \"_split\",\n",
    "          F.when(F.col(\"_rand\") < 0.70, F.lit(\"train\"))\n",
    "           .when(F.col(\"_rand\") < 0.85, F.lit(\"val\"))\n",
    "           .otherwise(F.lit(\"test\"))\n",
    "      )\n",
    ")\n",
    "\n",
    "train_df = splits.filter(F.col(\"_split\") == \"train\").drop(\"_rand\", \"_split\")\n",
    "val_df   = splits.filter(F.col(\"_split\") == \"val\").drop(\"_rand\", \"_split\")\n",
    "test_df  = splits.filter(F.col(\"_split\") == \"test\").drop(\"_rand\", \"_split\")\n",
    "\n",
    "# Optional sanity check\n",
    "print(\"Train:\", train_df.count(), \"Val:\", val_df.count(), \"Test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c25487f-c2b2-4b77-b2b6-a40e6fc5c21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 3) WRITE SPLITS TO GOLD/features_v2 ===\n",
    "# Overwrite to keep paths stable while iterating\n",
    "\n",
    "out_path = f\"{gold_path}/features_v2\"\n",
    "\n",
    "train_out = f\"{out_path}/train\"\n",
    "val_out   = f\"{out_path}/val\"\n",
    "test_out  = f\"{out_path}/test\"\n",
    "\n",
    "(train_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(train_out))\n",
    "\n",
    "(val_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(val_out))\n",
    "\n",
    "(test_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdf21d5-f545-40ce-934e-b6c75de153d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 4) QUICK VERIFICATION ===\n",
    "def load_and_count(p):\n",
    "    df = spark.read.format(\"delta\").load(p)\n",
    "    return df, df.count()\n",
    "\n",
    "train_loaded, n_train = load_and_count(train_out)\n",
    "val_loaded,   n_val   = load_and_count(val_out)\n",
    "test_loaded,  n_test  = load_and_count(test_out)\n",
    "\n",
    "print(\"Split counts →\",\n",
    "      \"train:\", n_train,\n",
    "      \"val:\",   n_val,\n",
    "      \"test:\",  n_test,\n",
    "      \"total:\", n_train + n_val + n_test)\n",
    "\n",
    "# Peek a few rows to ensure schema/fields look right\n",
    "train_loaded.show(5, truncate=False)\n",
    "val_loaded.show(5, truncate=False)\n",
    "test_loaded.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b0536b-cd5c-4e4c-ae0c-9cd1f8975219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 5) SAVE SPLIT MANIFEST WITH COUNTS + PERCENTAGES ===\n",
    "total_records = n_train + n_val + n_test\n",
    "\n",
    "manifest_data = [\n",
    "    (\"train\", n_train, round((n_train / total_records) * 100, 2)),\n",
    "    (\"val\",   n_val,   round((n_val / total_records) * 100, 2)),\n",
    "    (\"test\",  n_test,  round((n_test / total_records) * 100, 2))\n",
    "]\n",
    "\n",
    "manifest = spark.createDataFrame(\n",
    "    manifest_data,\n",
    "    [\"split\", \"count\", \"percentage\"]\n",
    ")\n",
    "\n",
    "(manifest.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\")\n",
    "  .save(f\"{features_v2_base}/_manifest_counts\"))\n",
    "\n",
    "manifest.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888bf42f-e308-4b8d-9ce8-256cf314be77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# =========================================================\n",
    "# GOODREADS TEXT FEATURE EXTRACTION\n",
    "# =========================================================\n",
    "# Purpose:\n",
    "#   Load train split from feature_v2 (Gold layer)\n",
    "#   Work with review_text column for NLP feature engineering\n",
    "# =========================================================\n",
    "\n",
    "# === 1. CONFIGURE STORAGE ACCESS ===\n",
    "storage_acct = \"goodreadsreviews60107070\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_acct}.dfs.core.windows.net\",\n",
    "    \"W68YOwummMkTfxvE8uSyeHwSn2ISU3fxF43SpgTIU/zdUDmwquZ95QpaxDJnze6PRovNww3bWamU+AStmbtZLg==\"\n",
    ")\n",
    "\n",
    "container = \"lakehouse\"\n",
    "gold_path = f\"abfss://{container}@{storage_acct}.dfs.core.windows.net/gold\"\n",
    "train_path = f\"{gold_path}/features_v2/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33ebc52-90e7-466b-923d-beb715cfecd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 10480029\nroot\n |-- book_id: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- rating: integer (nullable = true)\n |-- review_text: string (nullable = true)\n |-- language_code: string (nullable = true)\n |-- n_votes: integer (nullable = true)\n |-- date_added: string (nullable = true)\n |-- date_added_parsed: timestamp (nullable = true)\n |-- date_added_iso: date (nullable = true)\n |-- review_length: integer (nullable = true)\n |-- word_count: integer (nullable = true)\n |-- review_length_words: integer (nullable = true)\n |-- avg_rating_per_book: double (nullable = true)\n |-- n_reviews_per_book: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# === 2. LOAD DATASET (feature_v2/train) ===\n",
    "train_df = spark.read.format(\"delta\").load(train_path)\n",
    "\n",
    "print(\"Total records:\", train_df.count())\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0240c5d-f423-4c1a-812b-1fe21c8e1065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 3. TEXT CLEANING & NORMALIZATION (review_text) ===\n",
    "# If not already available on the cluster, install once:\n",
    "%pip install emoji==2.14.0\n",
    "\n",
    "import re\n",
    "from pyspark.sql.functions import col, length, lower, regexp_replace, trim\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- 3.1 Define regex patterns (Java/PCRE compatible for Spark) ---\n",
    "URL_PATTERN   = r'(https?://\\S+|www\\.\\S+)'\n",
    "NUM_PATTERN   = r'\\d+'\n",
    "# Remove punctuation EXCEPT < and > so placeholders like <URL> survive\n",
    "PUNCT_EXCEPT_PLACEHOLDERS = r'[\\\\p{Punct}&&[^<>]]'\n",
    "\n",
    "# --- 3.2 Emoji replacement via Python UDF (uses emoji lib) ---\n",
    "# Databricks note: ensure `emoji` package is installed on the cluster\n",
    "try:\n",
    "    import emoji\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install the `emoji` package on the cluster: %pip install emoji==2.14.0\") from e\n",
    "\n",
    "def replace_emojis_to_placeholder(text: str) -> str:\n",
    "    if text is None:\n",
    "        return None\n",
    "    # replace each emoji grapheme with <EMOJI>\n",
    "    # emoji.replace_emoji handles combined emojis and skin tones correctly\n",
    "    return emoji.replace_emoji(text, replace='<EMOJI>')\n",
    "\n",
    "replace_emojis_udf = F.udf(replace_emojis_to_placeholder, returnType=F.StringType())\n",
    "\n",
    "# --- 3.3 Apply cleaning pipeline in order ---\n",
    "# Order matters: placeholders first, then punctuation/spacing, then trim + filter\n",
    "cleaned_df = (\n",
    "    train_df\n",
    "      .withColumn(\"raw_text\", col(\"review_text\"))\n",
    "      # lowercase\n",
    "      .withColumn(\"clean_text\", lower(col(\"review_text\")))\n",
    "      # URLs -> <URL>\n",
    "      .withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), URL_PATTERN, \" <URL> \"))\n",
    "      # numbers -> <NUM>\n",
    "      .withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), NUM_PATTERN, \" <NUM> \"))\n",
    "      # emojis -> <EMOJI> (UDF)\n",
    "      .withColumn(\"clean_text\", replace_emojis_udf(col(\"clean_text\")))\n",
    "      # remove punctuation except <> to keep placeholders\n",
    "      .withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), PUNCT_EXCEPT_PLACEHOLDERS, \" \"))\n",
    "      # collapse multiple spaces\n",
    "      .withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), r\"\\s+\", \" \"))\n",
    "      # trim\n",
    "      .withColumn(\"clean_text\", trim(col(\"clean_text\")))\n",
    "      # filter out empty or very short reviews (<10 chars)\n",
    "      .filter(length(col(\"clean_text\")) >= 10)\n",
    ")\n",
    "\n",
    "# --- 3.4 Quick sanity checks ---\n",
    "print(\"After cleaning:\", cleaned_df.count())\n",
    "display(\n",
    "    cleaned_df.select(\"review_id\", \"raw_text\", \"clean_text\").limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb64ea5e-9f2d-4fbc-8cb6-c9f70af80599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 3.5 SAVE CLEANED TEXT (ALL COLUMNS) ===\n",
    "cleaned_out_path = f\"{gold_path}/features_v2/text_cleaned\"\n",
    "\n",
    "(\n",
    "    cleaned_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .save(cleaned_out_path)\n",
    ")\n",
    "\n",
    "print(f\"Full cleaned dataset saved to: {cleaned_out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049f6802-41a5-4bd5-a01b-4bf9c1694f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === III 4a. BASIC TEXT FEATURES ===\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "text_basic_df = (\n",
    "    cleaned_df\n",
    "    .withColumn(\"review_length_words\", F.size(F.split(F.col(\"clean_text\"), r\"\\s+\")))\n",
    "    .withColumn(\"review_length_chars\", F.length(F.col(\"clean_text\")))\n",
    "    .filter(F.col(\"review_length_words\") > 0)\n",
    ")\n",
    "\n",
    "# Quick sample\n",
    "display(\n",
    "    text_basic_df.select(\"review_id\",\"clean_text\",\"review_length_words\",\"review_length_chars\").limit(10)\n",
    ")\n",
    "\n",
    "# Summary (use percentile_approx for median)\n",
    "summary_df = (\n",
    "    text_basic_df.agg(\n",
    "        F.count(\"*\").alias(\"n_rows\"),\n",
    "        F.avg(\"review_length_words\").alias(\"avg_words\"),\n",
    "        F.percentile_approx(\"review_length_words\", 0.5).alias(\"p50_words\"),\n",
    "        F.max(\"review_length_words\").alias(\"max_words\"),\n",
    "        F.avg(\"review_length_chars\").alias(\"avg_chars\"),\n",
    "        F.max(\"review_length_chars\").alias(\"max_chars\"),\n",
    "    )\n",
    ")\n",
    "display(summary_df)\n",
    "\n",
    "# Save (ALL columns retained + new features); path aligned with earlier convention\n",
    "basic_out = f\"{gold_path}/features_v2/text_basic\"\n",
    "(text_basic_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(basic_out))\n",
    "\n",
    "print(\"Basic text features saved to:\", basic_out)\n",
    "\n",
    "# Optional reload\n",
    "reloaded = spark.read.format(\"delta\").load(basic_out)\n",
    "reloaded.printSchema()\n",
    "print(\"Count:\", reloaded.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5ee7ff-9bb7-416a-9a3c-8973a9d6ed0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === 4b. SENTIMENT FEATURES (VADER) ===\n",
    "# If not installed on cluster:\n",
    "%pip install nltk==3.9.1\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "# Download VADER lexicon if not already available\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# Initialize analyzer once (broadcast to executors)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define function to compute sentiment scores\n",
    "def vader_scores(text):\n",
    "    if text is None:\n",
    "        return (0.0, 0.0, 0.0, 0.0)\n",
    "    s = sia.polarity_scores(text)\n",
    "    return (float(s[\"pos\"]), float(s[\"neu\"]), float(s[\"neg\"]), float(s[\"compound\"]))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sentiment_pos\", DoubleType(), True),\n",
    "    StructField(\"sentiment_neu\", DoubleType(), True),\n",
    "    StructField(\"sentiment_neg\", DoubleType(), True),\n",
    "    StructField(\"sentiment_compound\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "vader_udf = F.udf(vader_scores, schema)\n",
    "\n",
    "# Apply to dataset\n",
    "sentiment_df = (\n",
    "    text_basic_df\n",
    "    .withColumn(\"sentiment\", vader_udf(F.col(\"clean_text\")))\n",
    "    .withColumn(\"sentiment_pos\", F.col(\"sentiment.sentiment_pos\"))\n",
    "    .withColumn(\"sentiment_neu\", F.col(\"sentiment.sentiment_neu\"))\n",
    "    .withColumn(\"sentiment_neg\", F.col(\"sentiment.sentiment_neg\"))\n",
    "    .withColumn(\"sentiment_compound\", F.col(\"sentiment.sentiment_compound\"))\n",
    "    .drop(\"sentiment\")\n",
    ")\n",
    "\n",
    "# Quick inspection\n",
    "display(\n",
    "    sentiment_df.select(\n",
    "        \"review_id\", \"clean_text\",\n",
    "        \"sentiment_pos\", \"sentiment_neu\", \"sentiment_neg\", \"sentiment_compound\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "# Save sentiment-enriched data\n",
    "sentiment_out = f\"{gold_path}/features_v2/text_sentiment\"\n",
    "(sentiment_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(sentiment_out))\n",
    "\n",
    "print(\"Sentiment features saved to:\", sentiment_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed8af9b-c63a-48b3-bc7c-6facc110b370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- book_id: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- rating: integer (nullable = true)\n |-- review_text: string (nullable = true)\n |-- language_code: string (nullable = true)\n |-- n_votes: integer (nullable = true)\n |-- date_added: string (nullable = true)\n |-- date_added_parsed: timestamp (nullable = true)\n |-- date_added_iso: date (nullable = true)\n |-- review_length: integer (nullable = true)\n |-- word_count: integer (nullable = true)\n |-- review_length_words: integer (nullable = true)\n |-- avg_rating_per_book: double (nullable = true)\n |-- n_reviews_per_book: long (nullable = true)\n |-- raw_text: string (nullable = true)\n |-- clean_text: string (nullable = true)\n |-- review_length_chars: integer (nullable = true)\n |-- sentiment_pos: double (nullable = true)\n |-- sentiment_neu: double (nullable = true)\n |-- sentiment_neg: double (nullable = true)\n |-- sentiment_compound: double (nullable = true)\n\nTotal records: 10442623\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>book_id</th><th>review_id</th><th>title</th><th>author_id</th><th>name</th><th>user_id</th><th>rating</th><th>review_text</th><th>language_code</th><th>n_votes</th><th>date_added</th><th>date_added_parsed</th><th>date_added_iso</th><th>review_length</th><th>word_count</th><th>review_length_words</th><th>avg_rating_per_book</th><th>n_reviews_per_book</th><th>raw_text</th><th>clean_text</th><th>review_length_chars</th><th>sentiment_pos</th><th>sentiment_neu</th><th>sentiment_neg</th><th>sentiment_compound</th></tr></thead><tbody><tr><td>18375252</td><td>bec7e62ae812fada353b8456d2371a87</td><td>Au Revoir Là-haut</td><td>822613</td><td>Pierre Lemaitre</td><td>c55bde87d5dce88e7b7a4ba5a4d2257d</td><td>1</td><td>banalities on banalities old tunes nothing new. dumped.</td><td>fre</td><td>0</td><td>Sun Mar 05 01:41:18 -0800 2017</td><td>2017-03-05T09:41:18Z</td><td>2017-03-05</td><td>55</td><td>8</td><td>16</td><td>4.225806451612903</td><td>31</td><td>banalities on banalities old tunes nothing new. dumped.</td><td>ba ali ies o ba ali ies old es o hi g ew. d m ed.</td><td>49</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><td>41865</td><td>9f2ca2cc167c2f16892858c09fe51b7c</td><td>Twilight (twilight, #1)</td><td>941441</td><td>Stephenie Meyer</td><td>3b92b0352627e473e429e80ff1ef7dd5</td><td>2</td><td>this is a book made for teenagers. the whole notion that you can have a relationship based on platonic love is very appealing for teenagers, specially girls, that feel targeted for their looks only. of course let's put in the mix some seriously sexy pretty vampire, that is rich and all powerful and that adores and is crazy about the main girl character, and the cocktail is just perfect. it is not very understandable this all powerful, pretty vampire is so crazy about this boringly plain girl, unless you read the book and realize he kind of didn't have much to choose from....  all said, of course we all have a teenager in our hearts and this is a very easy book to read. unless you get bored by teenager angst.</td><td>en-us</td><td>0</td><td>Tue Sep 10 00:39:23 -0700 2013</td><td>2013-09-10T07:39:23Z</td><td>2013-09-10</td><td>717</td><td>131</td><td>177</td><td>3.4414496264889967</td><td>9906</td><td>this is a book made for teenagers. the whole notion that you can have a relationship based on platonic love is very appealing for teenagers, specially girls, that feel targeted for their looks only. of course let's put in the mix some seriously sexy pretty vampire, that is rich and all powerful and that adores and is crazy about the main girl character, and the cocktail is just perfect. it is not very understandable this all powerful, pretty vampire is so crazy about this boringly plain girl, unless you read the book and realize he kind of didn't have much to choose from....  all said, of course we all have a teenager in our hearts and this is a very easy book to read. unless you get bored by teenager angst.</td><td>his is a book made for ee agers. he whole o io ha yo a have a rela io shi based o la o i love is very a eali g for ee agers, s e ially girls, ha feel arge ed for heir looks o ly. of o rse le 's i he mix some serio sly sexy re y vam ire, ha is ri h a d all owerf l a d ha adores a d is razy abo he mai girl hara er, a d he o k ail is j s erfe . i is o very ders a dable his all owerf l, re y vam ire is so razy abo his bori gly lai girl, less yo read he book a d realize he ki d of did ' have m h o hoose from.... all said, of o rse we all have a ee ager i o r hear s a d his is a very easy book o read. less yo ge bored by ee ager a gs .</td><td>637</td><td>0.187</td><td>0.8</td><td>0.013</td><td>0.9743</td></tr><tr><td>30109238</td><td>804b10dcc6b047605074a5c833373384</td><td>Lake Of Dreams (fortune Bay, Prequel Novella)</td><td>15240324</td><td>Judith  Hudson</td><td>600c811b96fed8dd0181b7024aac0524</td><td>5</td><td>very nice start to a series. love the setting of the book. looking forward to reading more.</td><td>eng</td><td>0</td><td>Thu Sep 08 11:35:19 -0700 2016</td><td>2016-09-08T18:35:19Z</td><td>2016-09-08</td><td>91</td><td>17</td><td>23</td><td>4.4</td><td>5</td><td>very nice start to a series. love the setting of the book. looking forward to reading more.</td><td>very i e s ar o a series. love he se i g of he book. looki g forward o readi g more.</td><td>84</td><td>0.271</td><td>0.729</td><td>0.0</td><td>0.6666</td></tr><tr><td>15803173</td><td>281289cad67a54610e731307e42bf728</td><td>Golden Boy</td><td>4818033</td><td>Abigail Tarttelin</td><td>17aaae5b58b453a8cdd4bc54c2ff3f0b</td><td>5</td><td>great book! so much food for thought regarding gender and identity... this book is powerful and touching.</td><td>eng</td><td>0</td><td>Sat Nov 21 23:16:47 -0800 2015</td><td>2015-11-22T07:16:47Z</td><td>2015-11-22</td><td>105</td><td>17</td><td>28</td><td>4.32258064516129</td><td>279</td><td>great book! so much food for thought regarding gender and identity... this book is powerful and touching.</td><td>grea book! so m h food for ho gh regardi g ge der a d ide i y... his book is owerf l a d o hi g.</td><td>96</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><td>1158706</td><td>e737dd555f23e1eaf45647e0cfaeb297</td><td>Strangers In Death (in Death, #26)</td><td>17065</td><td>J.d. Robb</td><td>fca26c34be8fe623ee340061f1281796</td><td>4</td><td>strangers in death (police proc-eve dallas-nyc-2060) - vg  robb, j.d. (aka nora roberts) - 26th in series  g.p. putnam's sons, 2008, us hardcover - isbn: 9780399154706  first sentence: murder harbored no bigotry, no bias.  when a wealthy man is found murdered in his apartment, lt. eve dallas first looks to the wife as a suspect. the wife, however, was out of the country with friends and has an air-tight alibi. dallas has a feeling, however, and a determination to find justice for the victim.  i have long admitted to being of fan of this series and this book doesn't change that. the strengths are all there; crisp dialogue with wonderful interjections of humor, wonderful characters and the portrayal of the relationship between them, the fun slightly-futuristic-but-not-unbelievable technology and, yes, some nice scenes between eve and her husband, roarke. the plot didn't have the same emotional charge some have had, but it did have a delightfully twisted villain. a slight weakness was whomever relied on spell-check to catch errors (hear versus here), but that's minor. somewhat more disappointing was that i saw where the plot was going a bit earlier than i'd have liked. however, that didn't prevent my reading the book all in one day and enjoying it.</td><td>eng</td><td>0</td><td>Tue Mar 11 22:21:13 -0700 2008</td><td>2008-03-12T05:21:13Z</td><td>2008-03-12</td><td>1265</td><td>208</td><td>324</td><td>3.953488372093023</td><td>129</td><td>strangers in death (police proc-eve dallas-nyc-2060) - vg  robb, j.d. (aka nora roberts) - 26th in series  g.p. putnam's sons, 2008, us hardcover - isbn: 9780399154706  first sentence: murder harbored no bigotry, no bias.  when a wealthy man is found murdered in his apartment, lt. eve dallas first looks to the wife as a suspect. the wife, however, was out of the country with friends and has an air-tight alibi. dallas has a feeling, however, and a determination to find justice for the victim.  i have long admitted to being of fan of this series and this book doesn't change that. the strengths are all there; crisp dialogue with wonderful interjections of humor, wonderful characters and the portrayal of the relationship between them, the fun slightly-futuristic-but-not-unbelievable technology and, yes, some nice scenes between eve and her husband, roarke. the plot didn't have the same emotional charge some have had, but it did have a delightfully twisted villain. a slight weakness was whomever relied on spell-check to catch errors (hear versus here), but that's minor. somewhat more disappointing was that i saw where the plot was going a bit earlier than i'd have liked. however, that didn't prevent my reading the book all in one day and enjoying it.</td><td>s ra gers i dea h ( oli e ro -eve dallas- y - <NUM> ) - vg robb, j.d. (aka ora rober s) - <NUM> h i series g. . am's so s, <NUM> , s hard over - isb : <NUM> firs se e e: m rder harbored o bigo ry, o bias. whe a weal hy ma is fo d m rdered i his a ar me , l . eve dallas firs looks o he wife as a s s e . he wife, however, was o of he o ry wi h frie ds a d has a air- igh alibi. dallas has a feeli g, however, a d a de ermi a io o fi d j s i e for he vi im. i have lo g admi ed o bei g of fa of his series a d his book does ' ha ge ha . he s re g hs are all here; ris dialog e wi h wo derf l i erje io s of h mor, wo derf l hara ers a d he or rayal of he rela io shi be wee hem, he f sligh ly-f ris i -b - o - believable e h ology a d, yes, some i e s e es be wee eve a d her h sba d, roarke. he lo did ' have he same emo io al harge some have had, b i did have a deligh f lly wis ed villai . a sligh weak ess was whomever relied o s ell- he k o a h errors (hear vers s here), b ha 's mi or. somewha more disa oi i g was ha i saw where he lo was goi g a bi earlier ha i'd have liked. however, ha did ' reve my readi g he book all i o e day a d e joyi g i .</td><td>1155</td><td>0.089</td><td>0.873</td><td>0.037</td><td>0.8894</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "18375252",
         "bec7e62ae812fada353b8456d2371a87",
         "Au Revoir Là-haut",
         "822613",
         "Pierre Lemaitre",
         "c55bde87d5dce88e7b7a4ba5a4d2257d",
         1,
         "banalities on banalities old tunes nothing new. dumped.",
         "fre",
         0,
         "Sun Mar 05 01:41:18 -0800 2017",
         "2017-03-05T09:41:18Z",
         "2017-03-05",
         55,
         8,
         16,
         4.225806451612903,
         31,
         "banalities on banalities old tunes nothing new. dumped.",
         "ba ali ies o ba ali ies old es o hi g ew. d m ed.",
         49,
         0.0,
         1.0,
         0.0,
         0.0
        ],
        [
         "41865",
         "9f2ca2cc167c2f16892858c09fe51b7c",
         "Twilight (twilight, #1)",
         "941441",
         "Stephenie Meyer",
         "3b92b0352627e473e429e80ff1ef7dd5",
         2,
         "this is a book made for teenagers. the whole notion that you can have a relationship based on platonic love is very appealing for teenagers, specially girls, that feel targeted for their looks only. of course let's put in the mix some seriously sexy pretty vampire, that is rich and all powerful and that adores and is crazy about the main girl character, and the cocktail is just perfect. it is not very understandable this all powerful, pretty vampire is so crazy about this boringly plain girl, unless you read the book and realize he kind of didn't have much to choose from....  all said, of course we all have a teenager in our hearts and this is a very easy book to read. unless you get bored by teenager angst.",
         "en-us",
         0,
         "Tue Sep 10 00:39:23 -0700 2013",
         "2013-09-10T07:39:23Z",
         "2013-09-10",
         717,
         131,
         177,
         3.4414496264889967,
         9906,
         "this is a book made for teenagers. the whole notion that you can have a relationship based on platonic love is very appealing for teenagers, specially girls, that feel targeted for their looks only. of course let's put in the mix some seriously sexy pretty vampire, that is rich and all powerful and that adores and is crazy about the main girl character, and the cocktail is just perfect. it is not very understandable this all powerful, pretty vampire is so crazy about this boringly plain girl, unless you read the book and realize he kind of didn't have much to choose from....  all said, of course we all have a teenager in our hearts and this is a very easy book to read. unless you get bored by teenager angst.",
         "his is a book made for ee agers. he whole o io ha yo a have a rela io shi based o la o i love is very a eali g for ee agers, s e ially girls, ha feel arge ed for heir looks o ly. of o rse le 's i he mix some serio sly sexy re y vam ire, ha is ri h a d all owerf l a d ha adores a d is razy abo he mai girl hara er, a d he o k ail is j s erfe . i is o very ders a dable his all owerf l, re y vam ire is so razy abo his bori gly lai girl, less yo read he book a d realize he ki d of did ' have m h o hoose from.... all said, of o rse we all have a ee ager i o r hear s a d his is a very easy book o read. less yo ge bored by ee ager a gs .",
         637,
         0.187,
         0.8,
         0.013,
         0.9743
        ],
        [
         "30109238",
         "804b10dcc6b047605074a5c833373384",
         "Lake Of Dreams (fortune Bay, Prequel Novella)",
         "15240324",
         "Judith  Hudson",
         "600c811b96fed8dd0181b7024aac0524",
         5,
         "very nice start to a series. love the setting of the book. looking forward to reading more.",
         "eng",
         0,
         "Thu Sep 08 11:35:19 -0700 2016",
         "2016-09-08T18:35:19Z",
         "2016-09-08",
         91,
         17,
         23,
         4.4,
         5,
         "very nice start to a series. love the setting of the book. looking forward to reading more.",
         "very i e s ar o a series. love he se i g of he book. looki g forward o readi g more.",
         84,
         0.271,
         0.729,
         0.0,
         0.6666
        ],
        [
         "15803173",
         "281289cad67a54610e731307e42bf728",
         "Golden Boy",
         "4818033",
         "Abigail Tarttelin",
         "17aaae5b58b453a8cdd4bc54c2ff3f0b",
         5,
         "great book! so much food for thought regarding gender and identity... this book is powerful and touching.",
         "eng",
         0,
         "Sat Nov 21 23:16:47 -0800 2015",
         "2015-11-22T07:16:47Z",
         "2015-11-22",
         105,
         17,
         28,
         4.32258064516129,
         279,
         "great book! so much food for thought regarding gender and identity... this book is powerful and touching.",
         "grea book! so m h food for ho gh regardi g ge der a d ide i y... his book is owerf l a d o hi g.",
         96,
         0.0,
         1.0,
         0.0,
         0.0
        ],
        [
         "1158706",
         "e737dd555f23e1eaf45647e0cfaeb297",
         "Strangers In Death (in Death, #26)",
         "17065",
         "J.d. Robb",
         "fca26c34be8fe623ee340061f1281796",
         4,
         "strangers in death (police proc-eve dallas-nyc-2060) - vg  robb, j.d. (aka nora roberts) - 26th in series  g.p. putnam's sons, 2008, us hardcover - isbn: 9780399154706  first sentence: murder harbored no bigotry, no bias.  when a wealthy man is found murdered in his apartment, lt. eve dallas first looks to the wife as a suspect. the wife, however, was out of the country with friends and has an air-tight alibi. dallas has a feeling, however, and a determination to find justice for the victim.  i have long admitted to being of fan of this series and this book doesn't change that. the strengths are all there; crisp dialogue with wonderful interjections of humor, wonderful characters and the portrayal of the relationship between them, the fun slightly-futuristic-but-not-unbelievable technology and, yes, some nice scenes between eve and her husband, roarke. the plot didn't have the same emotional charge some have had, but it did have a delightfully twisted villain. a slight weakness was whomever relied on spell-check to catch errors (hear versus here), but that's minor. somewhat more disappointing was that i saw where the plot was going a bit earlier than i'd have liked. however, that didn't prevent my reading the book all in one day and enjoying it.",
         "eng",
         0,
         "Tue Mar 11 22:21:13 -0700 2008",
         "2008-03-12T05:21:13Z",
         "2008-03-12",
         1265,
         208,
         324,
         3.953488372093023,
         129,
         "strangers in death (police proc-eve dallas-nyc-2060) - vg  robb, j.d. (aka nora roberts) - 26th in series  g.p. putnam's sons, 2008, us hardcover - isbn: 9780399154706  first sentence: murder harbored no bigotry, no bias.  when a wealthy man is found murdered in his apartment, lt. eve dallas first looks to the wife as a suspect. the wife, however, was out of the country with friends and has an air-tight alibi. dallas has a feeling, however, and a determination to find justice for the victim.  i have long admitted to being of fan of this series and this book doesn't change that. the strengths are all there; crisp dialogue with wonderful interjections of humor, wonderful characters and the portrayal of the relationship between them, the fun slightly-futuristic-but-not-unbelievable technology and, yes, some nice scenes between eve and her husband, roarke. the plot didn't have the same emotional charge some have had, but it did have a delightfully twisted villain. a slight weakness was whomever relied on spell-check to catch errors (hear versus here), but that's minor. somewhat more disappointing was that i saw where the plot was going a bit earlier than i'd have liked. however, that didn't prevent my reading the book all in one day and enjoying it.",
         "s ra gers i dea h ( oli e ro -eve dallas- y - <NUM> ) - vg robb, j.d. (aka ora rober s) - <NUM> h i series g. . am's so s, <NUM> , s hard over - isb : <NUM> firs se e e: m rder harbored o bigo ry, o bias. whe a weal hy ma is fo d m rdered i his a ar me , l . eve dallas firs looks o he wife as a s s e . he wife, however, was o of he o ry wi h frie ds a d has a air- igh alibi. dallas has a feeli g, however, a d a de ermi a io o fi d j s i e for he vi im. i have lo g admi ed o bei g of fa of his series a d his book does ' ha ge ha . he s re g hs are all here; ris dialog e wi h wo derf l i erje io s of h mor, wo derf l hara ers a d he or rayal of he rela io shi be wee hem, he f sligh ly-f ris i -b - o - believable e h ology a d, yes, some i e s e es be wee eve a d her h sba d, roarke. he lo did ' have he same emo io al harge some have had, b i did have a deligh f lly wis ed villai . a sligh weak ess was whomever relied o s ell- he k o a h errors (hear vers s here), b ha 's mi or. somewha more disa oi i g was ha i saw where he lo was goi g a bi earlier ha i'd have liked. however, ha did ' reve my readi g he book all i o e day a d e joyi g i .",
         1155,
         0.089,
         0.873,
         0.037,
         0.8894
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "book_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "author_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "review_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "language_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_votes",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "date_added",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date_added_parsed",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "date_added_iso",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "review_length",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "word_count",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "review_length_words",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating_per_book",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "n_reviews_per_book",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "raw_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "clean_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_length_chars",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_pos",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_neu",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_neg",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_compound",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_df = spark.read.format(\"delta\").load(f\"{gold_path}/features_v2/text_sentiment\")\n",
    "sentiment_df.printSchema()\n",
    "print(\"Total records:\", sentiment_df.count())\n",
    "display(sentiment_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b725c8-5246-4660-858c-001339d6ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vocabulary size: 1000\nTF-IDF features saved to: abfss://lakehouse@goodreadsreviews60107070.dfs.core.windows.net/gold/features_v2/text_tfidf\nVocabulary saved to Delta: abfss://lakehouse@goodreadsreviews60107070.dfs.core.windows.net/gold/features_v2/_tfidf_vocab_sklearn\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_id</th><th>vector_length</th></tr></thead><tbody><tr><td>00213a878c5438e52ca638b95c7df6cd</td><td>1000</td></tr><tr><td>0048829855ac3a7b99053ca11f859cf9</td><td>1000</td></tr><tr><td>0073464460a9110021c299cc7bdf95ba</td><td>1000</td></tr><tr><td>009aa2d16f5c8891925157004333c751</td><td>1000</td></tr><tr><td>00c876900904f25d3a3b59dd3afdc5f6</td><td>1000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "00213a878c5438e52ca638b95c7df6cd",
         1000
        ],
        [
         "0048829855ac3a7b99053ca11f859cf9",
         1000
        ],
        [
         "0073464460a9110021c299cc7bdf95ba",
         1000
        ],
        [
         "009aa2d16f5c8891925157004333c751",
         1000
        ],
        [
         "00c876900904f25d3a3b59dd3afdc5f6",
         1000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "vector_length",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# III.4(c) TF-IDF FEATURES — scikit-learn + Pandas UDF (final, rubric-compliant)\n",
    "# =========================================================\n",
    "# Prereqs:\n",
    "#   - sentiment_df (or cleaned_df) has columns [\"review_id\", \"clean_text\"]\n",
    "#   - gold_path already defined (e.g., abfss://lakehouse@.../gold)\n",
    "\n",
    "# %pip install scikit-learn==1.5.2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd, pickle, json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Spark safety config\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "\n",
    "# --- PARAMETERS (recommended rubric defaults) ---\n",
    "MAX_FEATURES = 1000           # limit vocabulary size (top N words)\n",
    "NGRAM_RANGE  = (1, 2)         # unigrams + bigrams → captures short context (“not good”)\n",
    "STOP_WORDS   = \"english\"      # remove common filler words\n",
    "SAMPLE_ROWS  = 10_000         # sample size for fitting vocab\n",
    "REPARTITIONS = 200            # partition count for distributed transform\n",
    "\n",
    "# --- 1) Select necessary columns & rebalance partitions ---\n",
    "base_df = sentiment_df.select(\"review_id\", \"clean_text\").repartition(REPARTITIONS)\n",
    "\n",
    "# --- 2) Fit TF-IDF vocabulary on manageable sample (driver-safe) ---\n",
    "sample_pdf = base_df.select(\"clean_text\").limit(SAMPLE_ROWS).toPandas()\n",
    "sample_texts = sample_pdf[\"clean_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    stop_words=STOP_WORDS\n",
    ")\n",
    "tfidf.fit(sample_texts)\n",
    "\n",
    "vocab = tfidf.get_feature_names_out().tolist()\n",
    "print(\"TF-IDF vocabulary size:\", len(vocab))\n",
    "\n",
    "# --- 3) Broadcast fitted vectorizer to executors ---\n",
    "bc_tfidf = spark.sparkContext.broadcast(pickle.dumps(tfidf))\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def tfidf_transform_batch(texts: pd.Series) -> pd.Series:\n",
    "    vec = pickle.loads(bc_tfidf.value)\n",
    "    X = vec.transform(texts.fillna(\"\").astype(str))\n",
    "    # convert each row to dense float32 array\n",
    "    return pd.Series([row.astype(\"float32\").toarray().ravel().tolist() for row in X])\n",
    "\n",
    "# --- 4) Apply TF-IDF transformation across all rows ---\n",
    "tfidf_df = base_df.withColumn(\"tfidf_features\", tfidf_transform_batch(col(\"clean_text\")))\n",
    "\n",
    "# --- 5) Save TF-IDF vectors (compact array form) ---\n",
    "tfidf_out = f\"{gold_path}/features_v2/text_tfidf\"\n",
    "(\n",
    "    tfidf_df\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .save(tfidf_out)\n",
    ")\n",
    "print(\"TF-IDF features saved to:\", tfidf_out)\n",
    "\n",
    "# --- 6) Save vocabulary (Delta + JSON for reproducibility) ---\n",
    "vocab_delta_out = f\"{gold_path}/features_v2/_tfidf_vocab_sklearn\"\n",
    "(\n",
    "    spark.createDataFrame([(i, t) for i, t in enumerate(vocab)], [\"index\",\"term\"])\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .save(vocab_delta_out)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Vocabulary saved to Delta:\", vocab_delta_out)\n",
    "\n",
    "# --- 7) Sanity check ---\n",
    "display(tfidf_df.select(\"review_id\", F.size(\"tfidf_features\").alias(\"vector_length\")).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb59395-b07b-47db-b0fc-6d5b17c63d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary JSON saved to: /dbfs/mnt/lakehouse/gold/features_v2/tfidf_vocab.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "vocab_json_out = f\"/dbfs/mnt/lakehouse/gold/features_v2/tfidf_vocab.json\"\n",
    "os.makedirs(os.path.dirname(vocab_json_out), exist_ok=True)\n",
    "\n",
    "with open(vocab_json_out, \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "print(\"Vocabulary JSON saved to:\", vocab_json_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29004e5-405a-4f21-a9f2-44a473a2b4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary JSON saved to: abfss://lakehouse@goodreadsreviews60107070.dfs.core.windows.net/gold/features_v2/tfidf_vocab.json\n"
     ]
    }
   ],
   "source": [
    "# === Save vocab JSON to /gold/features_v2 safely ===\n",
    "import os, json\n",
    "\n",
    "# 1) Write locally (Databricks driver)\n",
    "os.makedirs(\"/dbfs/tmp\", exist_ok=True)\n",
    "local_vocab_path = \"/dbfs/tmp/vocab.json\"\n",
    "\n",
    "with open(local_vocab_path, \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "# 2) Copy to Azure Data Lake (ABFSS)\n",
    "dst_path = f\"{gold_path}/features_v2/tfidf_vocab.json\"\n",
    "dbutils.fs.cp(\"dbfs:/tmp/vocab.json\", dst_path, True)\n",
    "\n",
    "print(\"Vocabulary JSON saved to:\", dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04de28e8-0a34-4246-b1df-d4f7abae92d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Safety: moderate Arrow batches\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ff35e4-5094-4ecb-ad12-ca2e26ecdda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 10442623\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "# Use the sentiment-enriched table\n",
    "source_df = spark.read.format(\"delta\").load(f\"{gold_path}/features_v2/text_sentiment\") \\\n",
    "                     .select(\"review_id\",\"clean_text\") \\\n",
    "                     .dropna(subset=[\"clean_text\"])\n",
    "\n",
    "# Stable row index for chunking (order by review_id for determinism)\n",
    "w = W.orderBy(\"review_id\")\n",
    "indexed_df = source_df.withColumn(\"row_idx\", F.row_number().over(w) - 1).cache()\n",
    "total_rows = indexed_df.count()\n",
    "print(\"Total rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c97a1de-d8be-46b0-aac3-fcfb484e3f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers==2.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (2.7.0)\nRequirement already satisfied: transformers==4.44.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (4.44.2)\nRequirement already satisfied: torch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (2.9.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from sentence-transformers==2.7.0) (4.67.1)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers==2.7.0) (1.26.4)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers==2.7.0) (1.4.2)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers==2.7.0) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from sentence-transformers==2.7.0) (0.36.0)\nRequirement already satisfied: Pillow in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers==2.7.0) (10.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (3.15.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers==4.44.2) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers==4.44.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from transformers==4.44.2) (2025.11.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers==4.44.2) (2.32.2)\nRequirement already satisfied: safetensors>=0.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from transformers==4.44.2) (0.6.2)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from transformers==4.44.2) (0.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /databricks/python3/lib/python3.12/site-packages (from torch) (4.11.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (74.0.0)\nRequirement already satisfied: sympy>=1.13.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (1.13.1.3)\nRequirement already satisfied: triton==3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from torch) (3.5.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (1.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ecaa2f63-80ee-4c2b-b7b3-6571056510c8/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers==4.44.2) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers==4.44.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers==4.44.2) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers==4.44.2) (2024.6.2)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.7.0) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.7.0) (2.2.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers==2.7.0 transformers==4.44.2 torch\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "\n",
    "MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-d\n",
    "BATCH_SIZE   = 64\n",
    "USE_GPU      = False  # set True if your cluster has GPUs\n",
    "\n",
    "_model = None\n",
    "def _get_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        device = \"cuda\" if USE_GPU else \"cpu\"\n",
    "        _model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "    return _model\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def sbert_embed(texts: pd.Series) -> pd.Series:\n",
    "    m = _get_model()\n",
    "    embs = m.encode(\n",
    "        texts.fillna(\"\").astype(str).tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,  # cosine-ready\n",
    "    )\n",
    "    return pd.Series([e.astype(\"float32\").tolist() for e in embs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec55a03-c415-4f30-9b1f-12cade896188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emb_out = f\"{gold_path}/features_v2/text_embeddings_sbert\"\n",
    "\n",
    "# Create empty table if not exists (schema)\n",
    "empty_df = indexed_df.limit(0).withColumn(\"bert_embedding\", F.array().cast(ArrayType(FloatType())))\n",
    "(empty_df\n",
    "    .select(\"review_id\",\"clean_text\",\"bert_embedding\")\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\",\"true\")\n",
    "    .save(emb_out))\n",
    "\n",
    "# Track progress in DBFS\n",
    "progress_path = \"dbfs:/tmp/emb_progress.txt\"\n",
    "def write_progress(msg):\n",
    "    dbutils.fs.put(progress_path, msg, True)\n",
    "def read_progress():\n",
    "    try:\n",
    "        return dbutils.fs.head(progress_path)\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e330865-197d-497d-bfa5-7ed64102ed46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "final_df = spark.read.format(\"delta\").load(emb_out)\n",
    "print(\"Rows in embeddings table:\", final_df.count())\n",
    "display(final_df.select(\"review_id\", F.size(\"bert_embedding\").alias(\"dim\")).limit(10))\n",
    "\n",
    "# Optional: Z-ORDER by review_id if you’ll join/filter by it frequently (Databricks SQL / OPTIMIZE):\n",
    "# spark.sql(f\"OPTIMIZE delta.`{emb_out}` ZORDER BY (review_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45a3a10-c2ed-40c6-a70a-574a791dd0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned chunks: 35 of ~300000 rows\nProcessing chunk [0-299999] ...\nWrote 85 bytes.\nDone chunk [0-299999]\nProcessing chunk [300000-599999] ...\nWrote 101 bytes.\nDone chunk [300000-599999]\nProcessing chunk [600000-899999] ...\nWrote 117 bytes.\nDone chunk [600000-899999]\nProcessing chunk [900000-1199999] ...\nWrote 134 bytes.\nDone chunk [900000-1199999]\nProcessing chunk [1200000-1499999] ...\nWrote 152 bytes.\nDone chunk [1200000-1499999]\nProcessing chunk [1500000-1799999] ...\nWrote 170 bytes.\nDone chunk [1500000-1799999]\nProcessing chunk [1800000-2099999] ...\nWrote 188 bytes.\nDone chunk [1800000-2099999]\nProcessing chunk [2100000-2399999] ...\nWrote 206 bytes.\nDone chunk [2100000-2399999]\nProcessing chunk [2400000-2699999] ...\nWrote 224 bytes.\nDone chunk [2400000-2699999]\nProcessing chunk [2700000-2999999] ...\nWrote 242 bytes.\nDone chunk [2700000-2999999]\nProcessing chunk [3000000-3299999] ...\nWrote 260 bytes.\nDone chunk [3000000-3299999]\nProcessing chunk [3300000-3599999] ...\nWrote 278 bytes.\nDone chunk [3300000-3599999]\nProcessing chunk [3600000-3899999] ...\nWrote 296 bytes.\nDone chunk [3600000-3899999]\nProcessing chunk [3900000-4199999] ...\nWrote 314 bytes.\nDone chunk [3900000-4199999]\nProcessing chunk [4200000-4499999] ...\nWrote 332 bytes.\nDone chunk [4200000-4499999]\nProcessing chunk [4500000-4799999] ...\nWrote 350 bytes.\nDone chunk [4500000-4799999]\nProcessing chunk [4800000-5099999] ...\nWrote 368 bytes.\nDone chunk [4800000-5099999]\nProcessing chunk [5100000-5399999] ...\nWrote 386 bytes.\nDone chunk [5100000-5399999]\nProcessing chunk [5400000-5699999] ...\n"
     ]
    }
   ],
   "source": [
    "# --- Speed knobs ---\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # moderate shuffle fanout\n",
    "\n",
    "# SBERT params (keep model same)\n",
    "MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "USE_GPU      = False          # set True if you have GPUs\n",
    "BATCH_SIZE   = 128            # ↑ batch\n",
    "REPARTITIONS = 32             # fewer, larger partitions\n",
    "CHUNK_SIZE   = 300_000        # ↑ chunk size to reduce driver/job overhead\n",
    "\n",
    "# Ensure base/indexed DF prepared once (outside loop), cached:\n",
    "# indexed_df: columns [review_id, clean_text, row_idx] with row_idx = row_number()-1, cache() called.\n",
    "\n",
    "# UDF (ensure global cache works)\n",
    "_model = None\n",
    "def _get_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        device = \"cuda\" if USE_GPU else \"cpu\"\n",
    "        # Optional: cache model to DBFS to avoid repeated downloads across clusters\n",
    "        _model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "    return _model\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def sbert_embed(texts: pd.Series) -> pd.Series:\n",
    "    m = _get_model()\n",
    "    embs = m.encode(\n",
    "        texts.fillna(\"\").astype(str).tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return pd.Series([e.astype(\"float32\").tolist() for e in embs])\n",
    "\n",
    "# Destination (append mode)\n",
    "emb_out = f\"{gold_path}/features_v2/text_embeddings_sbert\"\n",
    "\n",
    "from math import ceil\n",
    "total_rows = indexed_df.count()\n",
    "num_chunks = ceil(total_rows / CHUNK_SIZE)\n",
    "print(f\"Planned chunks: {num_chunks} of ~{CHUNK_SIZE} rows\")\n",
    "\n",
    "for k in range(num_chunks):\n",
    "    start = k * CHUNK_SIZE\n",
    "    end   = min((k+1) * CHUNK_SIZE, total_rows) - 1\n",
    "\n",
    "    done = read_progress()\n",
    "    tag = f\"[{start}-{end}]\"\n",
    "    if tag in done:\n",
    "        print(f\"Skip chunk {tag}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing chunk {tag} ...\")\n",
    "    # No extra shuffle columns; single repartition once per chunk\n",
    "    chunk_df = (\n",
    "        indexed_df\n",
    "          .where((F.col(\"row_idx\") >= start) & (F.col(\"row_idx\") <= end))\n",
    "          .select(\"review_id\",\"clean_text\")\n",
    "          .repartition(REPARTITIONS)\n",
    "    )\n",
    "\n",
    "    emb_df = chunk_df.withColumn(\"bert_embedding\", sbert_embed(col(\"clean_text\")))\n",
    "\n",
    "    (emb_df\n",
    "        .select(\"review_id\",\"clean_text\",\"bert_embedding\")\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .save(emb_out))\n",
    "\n",
    "    write_progress(done + f\"{tag} \")\n",
    "    print(f\"Done chunk {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d88c504-4666-4d13-a851-2526738c8f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def add_extra_features(df):\n",
    "    # tokens + counts\n",
    "    df = df.withColumn(\"tokens\", F.split(F.col(\"clean_text\"), r\"\\s+\")) \\\n",
    "           .withColumn(\"word_count\", F.size(F.col(\"tokens\"))) \\\n",
    "           .filter(F.col(\"word_count\") > 0)\n",
    "\n",
    "    # unique words + type–token ratio (TTR)\n",
    "    df = df.withColumn(\"unique_tokens\", F.array_distinct(F.col(\"tokens\"))) \\\n",
    "           .withColumn(\"unique_word_count\", F.size(F.col(\"unique_tokens\"))) \\\n",
    "           .withColumn(\"ttr\", F.col(\"unique_word_count\") / F.col(\"word_count\"))\n",
    "\n",
    "    # average word length\n",
    "    df = df.withColumn(\"avg_word_len\",\n",
    "                       F.aggregate(\n",
    "                           F.transform(F.col(\"tokens\"), lambda x: F.length(x)),\n",
    "                           F.lit(0), lambda acc, x: acc + x\n",
    "                       ) / F.col(\"word_count\"))\n",
    "\n",
    "    # placeholder counts created during cleaning\n",
    "    df = df.withColumn(\"url_count\",  F.size(F.expr(\"filter(tokens, x -> x = '<URL>')\"))) \\\n",
    "           .withColumn(\"num_count\",  F.size(F.expr(\"filter(tokens, x -> x = '<NUM>')\"))) \\\n",
    "           .withColumn(\"emoji_count\",F.size(F.expr(\"filter(tokens, x -> x = '<EMOJI>')\")))\n",
    "\n",
    "    # negation markers\n",
    "    NEG_SET = F.array(F.lit(\"not\"), F.lit(\"no\"), F.lit(\"never\"))\n",
    "    df = df.withColumn(\"negation_count\",\n",
    "                       F.size(F.expr(\"filter(tokens, x -> array_contains({}, x))\".format(NEG_SET.sql))))\n",
    "\n",
    "    # exclamation / question counts on cleaned_text (if punctuation retained elsewhere, else 0)\n",
    "    df = df.withColumn(\"exclaim_count\", F.length(F.regexp_replace(F.col(\"clean_text\"), r\"[^!]\", \"\"))) \\\n",
    "           .withColumn(\"question_count\",F.length(F.regexp_replace(F.col(\"clean_text\"), r\"[^?]\", \"\")))\n",
    "\n",
    "    # elongated words (e.g., sooooo, niiiice): count tokens with any char repeated ≥3\n",
    "    df = df.withColumn(\"elongated_count\",\n",
    "                       F.size(F.expr(\"filter(tokens, x -> x rlike '(.)\\\\1{2,}')\")))\n",
    "\n",
    "    # punctuation density proxy (after cleaning many puncts may be gone; still keep)\n",
    "    df = df.withColumn(\"punct_chars\", F.length(F.regexp_replace(F.col(\"clean_text\"), r\"[A-Za-z0-9<>\\\\s]\", \"\"))) \\\n",
    "           .withColumn(\"punct_ratio\", F.when(F.length(\"clean_text\") > 0,\n",
    "                                             F.col(\"punct_chars\") / F.length(\"clean_text\")).otherwise(F.lit(0.0)))\n",
    "\n",
    "    # drop helpers; keep final set\n",
    "    keep = [\"review_id\",\"clean_text\",\"word_count\",\"unique_word_count\",\"ttr\",\"avg_word_len\",\n",
    "            \"url_count\",\"num_count\",\"emoji_count\",\"negation_count\",\n",
    "            \"exclaim_count\",\"question_count\",\"elongated_count\",\"punct_ratio\"]\n",
    "    return df.select(*keep)\n",
    "\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    src = f\"{gold_path}/features_v2/{split}_text_cleaned\"   # must exist\n",
    "    dst = f\"{gold_path}/features_v2/{split}_text_extra\"\n",
    "\n",
    "    base = spark.read.format(\"delta\").load(src).select(\"review_id\",\"clean_text\")\n",
    "    extra = add_extra_features(base)\n",
    "    (extra.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(dst))\n",
    "\n",
    "    print(f\"Wrote extra features → {dst}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0af9462b-e3ea-4345-a052-19a81d384a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def load_or_empty(path, cols_keep):\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        # keep only requested columns that exist\n",
    "        keep = [c for c in cols_keep if c in df.columns]\n",
    "        return df.select(*keep)\n",
    "    except Exception:\n",
    "        # return empty DF with the requested schema (if any)\n",
    "        return spark.createDataFrame([], schema=\",\".join([f\"{c} string\" for c in cols_keep]))  # will be inner-joined safely later\n",
    "\n",
    "def combine_split(split: str):\n",
    "    base_path = f\"{gold_path}/features_v2\"\n",
    "    # metadata (from original split folder)\n",
    "    meta_cols = [\"review_id\",\"book_id\",\"rating\"]\n",
    "    meta = spark.read.format(\"delta\").load(f\"{base_path}/{split}\").select(*meta_cols)\n",
    "\n",
    "    # features (each must contain review_id + its columns)\n",
    "    basic = load_or_empty(f\"{base_path}/{split}_text_basic\",\n",
    "                          [\"review_id\",\"review_length_words\",\"review_length_chars\"])\n",
    "    senti = load_or_empty(f\"{base_path}/{split}_text_sentiment\",\n",
    "                          [\"review_id\",\"sentiment_pos\",\"sentiment_neu\",\"sentiment_neg\",\"sentiment_compound\"])\n",
    "    extra = load_or_empty(f\"{base_path}/{split}_text_extra\",\n",
    "                          [\"review_id\",\"word_count\",\"unique_word_count\",\"ttr\",\"avg_word_len\",\n",
    "                           \"url_count\",\"num_count\",\"emoji_count\",\"negation_count\",\n",
    "                           \"exclaim_count\",\"question_count\",\"elongated_count\",\"punct_ratio\"])\n",
    "    tfidf = load_or_empty(f\"{base_path}/{split}_text_tfidf\",\n",
    "                          [\"review_id\",\"tfidf_features\"])\n",
    "    emb   = load_or_empty(f\"{base_path}/{split}_text_embeddings\",\n",
    "                          [\"review_id\",\"bert_embedding\"])\n",
    "\n",
    "    # inner-join on review_id to guarantee aligned rows with all features\n",
    "    joined = (meta\n",
    "              .join(basic, \"review_id\", \"inner\")\n",
    "              .join(senti, \"review_id\", \"inner\")\n",
    "              .join(extra, \"review_id\", \"left\")   # extra is optional; left-join to not drop rows\n",
    "              .join(tfidf, \"review_id\", \"inner\")\n",
    "              .join(emb,   \"review_id\", \"left\"))  # embeddings optional; left-join if not computed yet\n",
    "    # enforce final column order\n",
    "    front = [\"review_id\",\"book_id\",\"rating\",\n",
    "             \"review_length_words\",\"review_length_chars\",\n",
    "             \"sentiment_pos\",\"sentiment_neu\",\"sentiment_neg\",\"sentiment_compound\"]\n",
    "    extras = [c for c in [\"word_count\",\"unique_word_count\",\"ttr\",\"avg_word_len\",\n",
    "                          \"url_count\",\"num_count\",\"emoji_count\",\"negation_count\",\n",
    "                          \"exclaim_count\",\"question_count\",\"elongated_count\",\"punct_ratio\"] if c in joined.columns]\n",
    "    tails = [\"tfidf_features\"] + ([\"bert_embedding\"] if \"bert_embedding\" in joined.columns else [])\n",
    "    ordered_cols = [c for c in front if c in joined.columns] + extras + tails\n",
    "    out = joined.select(*ordered_cols)\n",
    "\n",
    "    # minimal sanity checks\n",
    "    assert out.select(\"review_id\").distinct().count() == out.count(), \"duplicate review_id after joins\"\n",
    "    assert out.count() > 0, f\"no rows produced for split={split}\"\n",
    "\n",
    "    dst = f\"{base_path}/{split}_allfeatures\"\n",
    "    (out.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(dst))\n",
    "    print(f\"Wrote → {dst} | rows={out.count()} | cols={len(out.columns)}\")\n",
    "\n",
    "for sp in [\"train\",\"val\",\"test\"]:\n",
    "    combine_split(sp)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "goodread_text_features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}